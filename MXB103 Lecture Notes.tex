%!TEX TS-program = xelatex
%!TEX options = -aux-directory=Debug -shell-escape -file-line-error -interaction=nonstopmode -halt-on-error -synctex=1 "%DOC%"

\documentclass{article}
\input{LaTeX-Submodule/template.tex}

% Additional packages & macros

% Header and footer
\newcommand{\unitName}{Introductory Computational Mathematics}
\newcommand{\unitTime}{Semester 2, 2022}
\newcommand{\unitCoordinator}{Dr Elliot Carr}
\newcommand{\documentAuthors}{Tarang Janawalkar}

\fancyhead[L]{\unitName}
\fancyhead[R]{\leftmark}
\fancyfoot[C]{\thepage}

% Copyright
\usepackage[
    type={CC},
    modifier={by-nc-sa},
    version={4.0},
    imagewidth={5em},
    hyphenation={raggedright}
]{doclicense}

\date{}

\begin{document}
%
\begin{titlepage}
    \vspace*{\fill}
    \begin{center}
        \LARGE{\textbf{\unitName}} \\[0.1in]
        \normalsize{\unitTime} \\[0.2in]
        \normalsize\textit{\unitCoordinator} \\[0.2in]
        \documentAuthors
    \end{center}
    \vspace*{\fill}
    \doclicenseThis
    \thispagestyle{empty}
\end{titlepage}
\newpage
%
\tableofcontents
\newpage
%
\section{Preliminaries}
\subsection{Errors}
Errors in calculations are a common problem in numerical analysis.
We can quantify the magnitude of such an error by two measures.
\begin{definition}[Absolute and relative error]
    Let \(\tilde{x}\) be an approximation of \(x\). Then the \textbf{absolute
        error} is given by
    \begin{equation*}
        \text{absolute error} = \abs*{\tilde{x}-x}.
    \end{equation*}
    The \textbf{relative error} is given by
    \begin{equation*}
        \text{relative error} = \frac{\abs*{\tilde{x}-x}}{\abs*{x}}.
    \end{equation*}
\end{definition}
It is important to realise that the absolute error can be
misleading when comparing different sizes of errors,
i.e., it is always small for small values of \(x\) and \(\tilde{x}\).
\subsection{Floating Point Arithmetic}
The set of real numbers \(\R\) contains uncountably many elements.
Computers have a limited number of bits, and can
therefore only represent a small subset of these elements.

The most common approximation of real arithmetic used in computers is known
as \textbf{floating point arithmetic}.
\begin{definition}[Floating point number system]
    A floating point number system \(\mathbb{F}\left( \beta,\: k,\: m,\: M \right)\)
    is a \textit{finite subset} of the real number system characterised by the parameters:
    \begin{itemize}
        \item \(\beta \in \N\): the base
        \item \(k \in \N\): the number of digits in the significand
        \item \(m \in \Z\): the minimum exponent
        \item \(M \in \Z\): the maximum exponent
    \end{itemize}
\end{definition}
\begin{definition}[Floating point numbers]
    The floating point numbers \(f \in \mathbb{F}\left( \beta,\: k,\: m,\: M \right)\)
    are real numbers expressible in the form
    \begin{equation*}
        f = \pm \left( d_1.d_2 d_3 \dots d_k \right)_\beta \times \beta^e
    \end{equation*}
    where \(e \in \Z\) is the \textbf{exponent} satisfying \(m \leq e \leq M\).
    The quantity \(d_1.d_2 d_3 \dots d_k\) is known as the \textbf{significand},
    where \(d_i\) are base-\(\beta\) digits, with \(d_1 \neq 0\) unless \(f = 0\),
    to ensure a unique representation of \(f\).
\end{definition}
Computers primarily use floating point number systems with base \(\beta = 2\) (binary),
other common bases include \(\beta = 10\)
(decimal\footnote{Note that for base-10, we do not need to include the subscript in the significand.})
and \(\beta = 16\) (hexadecimal).

To illustrate the finiteness of the floating point number system, consider
the following example:
\begin{align*}
    \mathbb{F}\left( 10,\: 3,\: -1,\: 1\right) & = \begin{aligned}[t]
                                                       \left\{\right. & 0,                                                                                                                \\
                                                                      & \pm 1.00 \times 10^{-1},\: &  & \pm 1.01 \times 10^{-1},\: &  & \dots,\: &  & \pm 9.99 \times 10^{-1},            \\
                                                                      & \pm 1.00 \times 10^0,\:    &  & \pm 1.01 \times 10^0,\:    &  & \dots,\: &  & \pm 9.99 \times 10^0,               \\
                                                                      & \pm 1.00 \times 10^1,\:    &  & \pm 1.01 \times 10^1,\:    &  & \dots,\: &  & \pm 9.99 \times 10^1 \left.\right\}
                                                   \end{aligned} \\
                                               & = \begin{aligned}[t]
                                                       \left\{\right. & 0,                                                                        \\
                                                                      & \pm 0.100,\: &  & \pm 0.101,\: &  & \dots,\: &  & \pm 0.999,              \\
                                                                      & \pm 1.00,\:  &  & \pm 1.01,\:  &  & \dots,\: &  & \pm 9.99,               \\
                                                                      & \pm 10.0,\:  &  & \pm 10.1,\:  &  & \dots,\: &  & \pm 99.9 \left.\right\}
                                                   \end{aligned}
\end{align*}
Note that the numbers in this set are not equally spaced, (smaller spacing for smaller exponents).
\begin{definition}[Overflow and underflow]
    Consider the value \(x \in \R\), if \(x\) is too small in magnitude to be
    represented in \(\mathbb{F}\),
    an \textbf{underflow} occurs which typically causes the number to be replaced by zero.

    Similarly, if \(x\) is too large in magnitude to be represented in \(\mathbb{F}\),
    an \textbf{overflow} occurs which typically causes the number to be replaced by infinity.
\end{definition}
\begin{corollary}
    The smallest and largest values (in magnitude) of \(\mathbb{F}\) are given by
    \begin{align*}
        \min_{f \in \mathbb{F}} \abs*{f} & = \beta^m                                      \\
        \max_{f \in \mathbb{F}} \abs*{f} & = \left( 1 - \beta^{-k} \right) \beta^{M + 1}.
    \end{align*}
    The cardinality of the positive elements in \(\mathbb{F}\),
    is given by
    \begin{equation*}
        \abs*{\left\{ f \in \mathbb{F} : f > 0 \right\}} = \left( M - m + 1 \right) \left( \beta - 1 \right) \beta^{k - 1}
    \end{equation*}
    so that by including negative numbers and zero, the cardinality of \(\mathbb{F}\)
    is given by
    \begin{equation*}
        \abs*{\mathbb{F}} = 2 \abs*{\left\{ f \in \mathbb{F} : f > 0 \right\}} + 1.
    \end{equation*}
\end{corollary}
\subsubsection{Representing Real Numbers as Floating Point Numbers}
If we wish to represent a real number\footnote{\(x\) must satisfy \(\min{\left( \mathbb{F} \right)} \leq x \leq \max{\left( \mathbb{F} \right)}\).}
\(x\) that is not exactly representable in \(\mathbb{F}\),
we can \textbf{round} the number to the nearest \textit{representable} number.

The error committed by this process is known as the \textbf{roundoff error}.
\subsubsection{Converting between Floating Point Number Systems}
Consider \(fl : \R \to \mathbb{F}\left( \beta,\: k,\: m,\: M \right)\), defined as function which maps
real numbers \(x\) to the nearest element in
\(\mathbb{F}\). To determine \(fl\left( x \right)\):
\begin{enumerate}
    \item Express \(x\) in base \(\beta\).
    \item Express \(x\) in scientific form.
    \item Verify that \(m \leq e \leq M\):
          \begin{itemize}
              \item If \(e > M\), then \(x = \infty\).
              \item If \(e < m\), then \(x = 0\).
              \item Otherwise, round the significand to \(k\) digits.
          \end{itemize}
\end{enumerate}
The relative error produced by rounding \(x\) to \(fl\left( x \right)\) is bounded
according to
\begin{equation*}
    \frac{\abs*{x - fl\left( x \right)}}{\abs*{x}} \leq \frac{1}{2} \beta^{1 - k}.
\end{equation*}
\begin{definition}[Unit roundoff]
    The \textbf{unit roundoff} or \textbf{machine precision} \(u\) of a floating point number system \(\mathbb{F}\left( \beta,\: k,\: m,\: M \right)\)
    is given by
    \begin{equation*}
        u = \frac{1}{2} \beta^{1 - k}.
    \end{equation*}
\end{definition}
\subsubsection{IEEE Floating Point Standard}
IEEE 754 is the standard for floating point arithmetic used by most modern computers.

It is a binary format, with several variants. The most common variant is \textbf{IEEE double precision},
which is based on \(\mathbb{F}\left( 2,\: 53,\: -1022,\: 1023 \right)\).

The basic properties of this format are summarised in the following table.
\begin{table}[H]
    \centering
    \begin{tabular}{l | c} % chktex 44
        \toprule
        Unit roundoff                          & \(u = 1.11 \times 10^{-16}\)               \\
        Largest representable positive number  & \(1.80 \times 10^{308}\)                   \\
        Smallest representable positive number & \(2.23 \times 10^{-308}\)                  \\
        Special values                         & \(\pm 0\), \(\pm \infty\), \lstinline!NaN! \\ % chktex 13
        \bottomrule
    \end{tabular}
    % \caption{} % \label{}
\end{table}
\subsection{Catastrophic Cancellation}
When working with floating point arithmetic, roundoff is a common source of error.
Certain operations may bring roundoff errors that are too large to be easily corrected.

\textbf{Catastrophic cancellation} or \textbf{cancellation error} is the error that occurs in the floating
point subtraction of two numbers that are very close to each other, where at least
one of them is not exactly representable.

As an example, the quadratic formula
\begin{align*}
    x_1 = \frac{-b + \sqrt{b^2 - 4 a c}}{2 a} &  & x_2 = \frac{-b - \sqrt{b^2 - 4 a c}}{2 a}
\end{align*}
experiences catastrophic cancellation for \(b^2 \gg 4ac\), as \(b^2 - 4ac \approx b^2\) so that
\(\sqrt{b^2 - 4ac} = \sqrt{b^2} = \abs*{b}\):
\begin{align*}
    x_1 = \frac{-b + \abs*{b}}{2 a} &  & x_2 = \frac{-b - \abs*{b}}{2 a}
\end{align*}
When \(b > 0\), \(\abs*{b} = b\), so that
\begin{equation*}
    x_1 = \frac{-b + b}{2a} = \frac{b - b}{2a}.
\end{equation*}
And when \(b < 0\), \(\abs*{b} = -b\), so that
\begin{equation*}
    x_2 = \frac{-b - \left( -b \right)}{2a} = \frac{b - b}{2a}.
\end{equation*}
This cancellation can be avoided by taking the product of the two roots to determine the exact result
of the root that suffers from catastrophic cancellation.
\begin{equation*}
    x_1 x_2 = \frac{c}{a}.
\end{equation*}
\subsection{Taylor Polynomials}
Suppose we have a function \(f\left( x \right)\) that is \(n\) differentiable at the point
\(x = x_0\). This function can be approximated by a sum of polynomials that agrees with its first \(n\) derivatives at that point.
\begin{definition}[Taylor polynomial]
    The \textbf{Taylor polynomial} of degree \(n\) of \(f\), centred at \(x_0\) is defined by
    \begin{align*}
        P_n\left( x \right) & = \sum_{k = 0}^n \frac{f^{\left( k \right)}\left( x_0 \right)}{k!} \left( x - x_0 \right)^k.
    \end{align*}
\end{definition}
The Taylor polynomial can be used to approximate a function \(f\) for values of \(x\) near \(x_0\), the following
theorem addresses how accurate the approximation is.
\begin{definition}[Taylor's theorem]
    Suppose that \(f\) is \(n + 1\) times differentiable on an interval \(\interval{a}{b}\) containing \(x_0\),
    and let \(P_n\) be the degree \(n\) Taylor polynomial for \(f\), centred on \(x_0\). Then for all \(x \in \interval{a}{b}\),
    there exists a value \(x_0 < c < x\) such that
    \begin{equation*}
        f\left( x \right) = P_n\left( x \right) + \frac{f^{\left( n + 1 \right)}\left( c \right)}{\left( n + 1 \right)!} \left( x - x_0 \right)^{n + 1}.
    \end{equation*}
    The term
    \begin{equation*}
        R_n\left( x \right) = \frac{f^{\left( n + 1 \right)}\left( c \right)}{\left( n + 1 \right)!} \left( x - x_0 \right)^{n + 1}
    \end{equation*}
    is called the \textbf{error term} or \textbf{remainder term} for \(P_n\).
\end{definition}
To determine the absolute error from the Taylor series polynomial, consider the error term:
\begin{equation*}
    \abs*{f\left( x \right) - P_n\left( x \right)} = \abs*{R_n\left( x \right)}.
\end{equation*}
The maximum value of \(\abs*{R_n\left( x \right)}\) on the interval \(\interval{a}{b}\) gives the bound on the maximum error incurred when approximating
\(f\) by \(P_n\) on that interval.
\subsection{Taylor Series}
Given an infinitely differentiable function \(f\), we can take the limit \(n \to \infty\) to find Taylor series representation of \(f\), given by:
\begin{equation*}
    f\left( x \right) = \sum_{k = 0}^\infty \frac{f^{\left( k \right)}\left( x_0 \right)}{k!} \left( x - x_0 \right)^k.
\end{equation*}
When we truncate this series at a finite \(n\), the error from the Taylor series is known as \textbf{truncation error}. In this case,
the remainder term gives us a \textit{bound} on the size of this truncation error.
\section{Ordinary Differential Equations}
For certain classes of ordinary differential equations (ODEs), we can obtain analytical closed-form solutions using known techniques.
However for most cases we will need to use numerical techniques to obtain approximate solutions.
\subsection{Initial Value Problems}
While solutions with arbitrary constants, such as \(y = C e^{t}\) are acceptable for theoretical analysis,
we cannot have such variables when determining an approximate solution in the real world.
Hence we require \textbf{initial conditions} to obtain a definitive solution. An ODE combined
with an initial condition is called an \textbf{initial value problem} (IVP).
\subsection{Time Discretisation}
Consider the initial value problem,
\begin{align*}
    y\left( a \right)          & = \alpha                                                              \\
    \odv{y\left( t \right)}{t} & = f\left( t,\: y\left( t \right) \right), \quad \quad a \leq t \leq b
\end{align*}
Divide the interval \(\interval{a}{b}\) into \(n\) subintervals, each with width \(h = \left( b - a \right) / n\).
Then define \(t_i = a + i h\), for \(i = 0,\: 1,\: \ldots,\: n\), so that \(t_0 = a\) and \(t_n = b\). If we then compute
\(y_i = y\left( t_i \right)\) denoted as \(w_i\), then \(w_i \approx y_i\) for all \(i = 0,\: 1,\: \ldots,\: n\).
\subsection{Euler's Method}
Euler's method, or the first order Taylor method, uses a Taylor polynomial approximation of \(y\) over each subinterval.
Assuming \(y\) is twice differentiable,
\begin{equation*}
    y\left( t_i + h \right) = y\left( t_i \right) + h y'\left( t_i \right) + \mathcal{O}\left( h^2 \right).
\end{equation*}
The remainder term is not shown in its exact form but rather as \(\mathcal{O}\left( h^2 \right)\), ``Big-O of \(h^2\)'', or ``order \(h^2\)'', meaning that
the error is proportional to \(h^2\). Using the ODE and substituting \(t_{i + 1} = t_i + h\) gives
\begin{equation*}
    y_{i + 1} = y_i + h f\left( t_i,\: y_i \right) + \mathcal{O}\left( h^2 \right).
\end{equation*}
By removing the remainder term, we obtain the approximation method known as \textbf{Euler's method}:
\begin{align*}
    w_0       & = \alpha                             \\
    w_{i + 1} & = w_i + h f\left( t_i,\: w_i \right)
\end{align*}
for \(i = 0,\: 1,\: \ldots,\: n - 1\).
\subsection{Local and Global Error}
\textbf{Local error} is defined as the error that the method would incur in \textbf{one step},
assuming the solution was correct at the previous step.
The \textbf{global error} is defined as the error in the solution after \(i\) steps (at \(t = t_i\)),
and is given by:
\begin{equation*}
    \abs*{y_i - w_i}.
\end{equation*}
It is the accumulation of the local errors from steps \(1,\: 2,\: \ldots,\: i\).

The \textbf{order} of a method refers to the global error of that method. For Euler's
method, the local error is \(\mathcal{O}\left( h^2 \right)\) while the global error is
\(\mathcal{O}\left( h \right)\), hence ``\textit{first order} Taylor method''.

In general, if the local error is \(\mathcal{O}\left( h^{p + 1} \right)\), then the global error is
\(\mathcal{O}\left( h^p \right)\) and the method is said to be a \(p\)th order method.
\begin{equation*}
    \text{Global error} \approx n \mathcal{O}\left( h^{p + 1} \right) = \frac{b - a}{h} \mathcal{O}\left( h^{p + 1} \right) = \mathcal{O}\left( h^p \right).
\end{equation*}
\subsection{Second Order Taylor Method}
To improve upon the accuracy of Euler's method, we can use additional Taylor polynomial terms by truncating at a higher order.
Assuming \(y\) is three times differentiable, we have
\begin{equation*}
    y\left( t_i + h \right) = y\left( t_i \right) + h y'\left( t_i \right) + \frac{h^2}{2} y''\left( t_i \right) + \mathcal{O}\left( h^3 \right)
\end{equation*}
which can be rewritten as
\begin{equation*}
    y_{i + 1} = y_i + h f\left( t_i,\: y_i \right) + \frac{h^2}{2} f'\left( t_i\: y_i \right) + \mathcal{O}\left( h^3 \right).
\end{equation*}
Again by removing the remainder term, we obtain the approximation \(w_i\) of \(y_i\), known as the \textbf{second order Taylor method}:
\begin{align*}
    w_0       & = \alpha                                                                         \\
    w_{i + 1} & = w_i + h f\left( t_i,\: w_i \right) + \frac{h^2}{2} f'\left( t_i,\: w_i \right)
\end{align*}
for \(i = 0,\: 1,\: \ldots,\: n - 1\).
This method has a local error of \(\mathcal{O}\left( h^3 \right)\), and therefore a global error of \(\mathcal{O}\left( h^2 \right)\).
\subsection{Modified Euler Method}
Although the second order Taylor method is the more accurate than Euler's method,
we require computing \(f'\left( t,\: y \right)\).

Suppose we use a numerical approximation of the derivative of \(f\).

By definition, the derivative of a function is the limiting value of the slope of the line connecting
two nearby points on a curve,
\begin{equation*}
    f'\left( t_i,\: y_i \right) = \lim_{h \to 0} \frac{f\left( t_{i + 1},\: y_{i + 1} \right) - f\left( t_i,\: y_i \right)}{h}.
\end{equation*}
For small values of \(h\), we can approximate the derivative with
\begin{equation*}
    f'\left( t_i,\: y_i \right) \approx \frac{f\left( t_{i + 1},\: y_{i + 1} \right) - f\left( t_i,\: y_i \right)}{h}.
\end{equation*}
By deriving the error term we find that
\begin{equation*}
    f'\left( t_i,\: y_i \right) = \frac{f\left( t_{i + 1},\: y_{i + 1} \right) - f\left( t_i,\: y_i \right)}{h} + \mathcal{O}\left( h \right).
\end{equation*}
Hence the second order Taylor polynomial becomes
\begin{align*}
    y_{i + 1} & = y_i + h f\left( t_i,\: y_i \right) + \frac{h^2}{2} f'\left( t_i,\: y_i \right) + \mathcal{O}\left( h^3 \right)                                                                                                \\
              & = y_i + h f\left( t_i,\: y_i \right) + \frac{h^2}{2} \left[ \frac{f\left( t_{i + 1},\: y_{i + 1} \right) - f\left( t_i,\: y_i \right)}{h} + \mathcal{O}\left( h \right) \right] + \mathcal{O}\left( h^3 \right) \\
              & = y_i + h f\left( t_i,\: y_i \right) + \frac{h}{2} f\left( t_{i + 1},\: y_{i + 1} \right) - \frac{h}{2} f\left( t_i,\: y_i \right) + \mathcal{O}\left( h^3 \right) + \mathcal{O}\left( h^3 \right)              \\
              & = y_i + \frac{h}{2} f\left( t_i,\: y_i \right) + \frac{h}{2} f\left( t_{i + 1},\: y_{i + 1} \right) + \mathcal{O}\left( h^3 \right)                                                                             \\
              & = y_i + \frac{h}{2} \left[ f\left( t_i,\: y_i \right) + f\left( t_{i + 1},\: y_{i + 1} \right) \right] + \mathcal{O}\left( h^3 \right)                                                                          \\
              & \approx y_i + \frac{h}{2} \left[ f\left( t_i,\: y_i \right) + f\left( t_{i + 1},\: y_{i + 1} \right) \right]                                                                                                    \\
    w_{i + 1} & = w_i + \frac{h}{2} \left[ f\left( t_i,\: w_i \right) + f\left( t_{i + 1},\: w_{i + 1} \right) \right]
\end{align*}
As this formula involves itself, we will use Euler's method on
\(w_{i + 1} = w_i + h f\left( t_i,\: w_i \right)\) on the RHS\@.
\begin{align*}
    w_{i + 1} & = w_i + \frac{1}{2} \left[ h f\left( t_i,\: w_i \right) + f\left( t_{i + 1},\: w_{i + 1} \right) \right]                                                                                     \\
              & = w_i + \frac{1}{2} \left[ \underbrace{h f\left( t_i,\: w_i \right)}_{k_1} + \underbrace{h f\left( t_{i + 1},\: w_i + \underbrace{h f\left( t_i,\: w_i \right)}_{k_1} \right)}_{k_2} \right] \\
              & = w_i + \frac{1}{2} \left( k_1 + k_2 \right)
\end{align*}
This result leads to what is known as the \textbf{modified Euler method}.
\begin{align*}
    w_0       & = \alpha                                     \\
    w_{i + 1} & = w_i + \frac{1}{2} \left( k_1 + k_2 \right) \\
    k_1       & = h f\left( t_i,\: w_i \right)               \\
    k_2       & = h f\left( t_i + h,\: w_i + k_1 \right)
\end{align*}
This method has a local error of \(\mathcal{O}\left( h^3 \right)\), and therefore a global error of \(\mathcal{O}\left( h^2 \right)\).
\subsection{Runge-Kutta Method}
Another popular method that agrees with the Taylor method for high orders is the Runge-Kutta method.
It is a fourth order method, referred to as RK4. It has the following form:
\begin{align*}
    w_0       & = \alpha                                                     \\
    w_{i + 1} & = w_i + \frac{1}{6} \left( k_1 + 2 k_2 + 2 k_3 + k_4 \right) \\
    k_1       & = h f\left( t_i,\: w_i \right)                               \\
    k_2       & = h f\left( t_i + \frac{h}{2},\: w_i + \frac{k_1}{2} \right) \\
    k_3       & = h f\left( t_i + \frac{h}{2},\: w_i + \frac{k_2}{2} \right) \\
    k_4       & = h f\left( t_i + h,\: w_i + k_3 \right)
\end{align*}
for \(i = 0,\: 1,\: \ldots,\: n - 1\). The local error is \(\mathcal{O}\left( h^5 \right)\), and the global error is \(\mathcal{O}\left( h^4 \right)\).
\section{Interpolation}
\begin{definition}[Interpolating function]
    A function \(f\) is said to interpolate the data points
    \begin{equation*}
        \left\{ \left( x_0,\: y_0 \right),\: \left( x_1,\: y_1 \right),\: \dots,\: \left( x_n,\: y_n \right) \right\}
    \end{equation*}
    if it satisfies \(f\left( x_0 \right) = y_0\), \(f\left( x_1 \right) = y_1\), \dots,  \(f\left( x_n \right) = y_n\).
\end{definition}
There are many kinds of functions that may satisfy an interpolating function, however, the most common choice is a
\textbf{polynomial}.

The following theorem addresses the existence and uniqueness of interpolating polynomials, given distinct \(x\)-values, or \textbf{abscissas}.
\begin{theorem}
    Let the abscissas \(x_0\), \(x_1\), \dots, \(x_n\) be distinct and the function values \(y_0\), \(y_1\), \dots, \(y_n\)
    be given. Then there exists a \textbf{unique} interpolating polynomial \(P_n\) of degree (at most) \(n\), such that
    \(P_n\left( x_i \right) = y_i\) for \(i \in \interval{0}{n}\).
\end{theorem}
The following methods are used to find the interpolating polynomial.
\subsection{Lagrange Form}
\subsubsection{Two Data Points}
Consider constructing the interpolating polynomial through the points \(\left\{ \left( x_0,\: y_0 \right),\: \left( x_1,\: y_1 \right) \right\}\).
We require a polynomial of degree \(1\) to satisfy the interpolating function. Let \(P_1\left( x \right) = a_0 + a_1 x\) satisfy the following
conditions:
\begin{align*}
    P_1\left( x_0 \right) & = y_0 & \iff a_0 + a_1 x_0 & = y_0 \\
    P_1\left( x_1 \right) & = y_1 & \iff a_0 + a_1 x_1 & = y_1
\end{align*}
solving this linear system of equations, we obtain the following:
\begin{align*}
    a_0 & = \frac{x_1 y_0 - x_0 y_1}{x_1 - x_0} \\
    a_1 & = \frac{y_1 - y_0}{x_1 - x_0}
\end{align*}
so that
\begin{equation*}
    P_1\left( x \right) = \frac{x_1 y_0 - x_0 y_1}{x_1 - x_0} + \frac{y_1 - y_0}{x_1 - x_0} x.
\end{equation*}
To generalise this process, consider the following rearrangement of the polynomial:
\begin{align*}
    P_1\left( x \right) & = \frac{x_1 y_0 - x_0 y_1}{x_1 - x_0} + \frac{y_1 - y_0}{x_1 - x_0} x                                                              \\
                        & = \frac{x_1 y_0}{x_1 - x_0} - \frac{x_0 y_1}{x_1 - x_0} + \frac{y_1 x}{x_1 - x_0} - \frac{y_0 x}{x_1 - x_0}                        \\
                        & = \left( \frac{-x}{x_1 - x_0} + \frac{x_1}{x_1 - x_0} \right) y_0 + \left( \frac{x}{x_1 - x_0} - \frac{x_0}{x_1 - x_0} \right) y_1 \\
                        & = \frac{x - x_1}{x_0 - x_1} y_0 + \frac{x - x_0}{x_1 - x_0} y_1.
\end{align*}
This form clearly shows that each point satisfies the interpolating function.
\begin{itemize}
    \item The coefficient of \(y_0\) is \textbf{1} when \(x = x_0\), and \textbf{0} when \(x = x_1\).
    \item The coefficient of \(y_1\) is \textbf{0} when \(x = x_0\), and \textbf{1} when \(x = x_1\).
\end{itemize}
we can then write \(P_1\left( x \right)\) as
\begin{equation*}
    P_1\left( x \right) = L_{1,\: 0}\left( x \right) y_0 + L_{1,\: 1}\left( x \right) y_1
\end{equation*}
where \(L_{1,\: 0}\left( x \right) = \frac{x - x_1}{x_0 - x_1}\) and \(L_{1,\: 1}\left( x \right) = \frac{x - x_0}{x_1 - x_0}\)
are the Lagrange basis functions. The first index corresponds to the degree
of the polynomial, and the second index corresponds to the index of the
point for which it is the coefficient of.
\subsubsection{Three Data Points}
For three data points, consider the reverse problem:
\begin{itemize}
    \item The coefficient of \(y_0\) is \textbf{1} when \(x = x_0\), \textbf{0} when \(x = x_1\), and \textbf{0} when \(x = x_2\).
    \item The coefficient of \(y_1\) is \textbf{0} when \(x = x_0\), \textbf{1} when \(x = x_1\), and \textbf{0} when \(x = x_2\).
    \item The coefficient of \(y_2\) is \textbf{0} when \(x = x_0\), \textbf{0} when \(x = x_1\), and \textbf{1} when \(x = x_2\).
\end{itemize}
then solve for the Lagrange basis functions:
\begin{align*}
    P_2\left( x \right) & = L_{2,\: 0}\left( x \right) y_0 + L_{2,\: 1}\left( x \right) y_1 + L_{2,\: 2}\left( x \right) y_2                                                                                                                                                                                                                                   \\
                        & = \frac{\left( x - x_1 \right)\left( x - x_2 \right)}{\left( x_0 - x_1 \right)\left( x_0 - x_2 \right)} y_0 + \frac{\left( x - x_0 \right)\left( x - x_2 \right)}{\left( x_1 - x_0 \right)\left( x_1 - x_2 \right)} y_1 + \frac{\left( x - x_0 \right)\left( x - x_1 \right)}{\left( x_2 - x_0 \right)\left( x_2 - x_1 \right)} y_2.
\end{align*}
\subsubsection{Arbitrary Number of Points}
Given the abscissas \(\left\{ x_0,\: x_1,\: \dots,\: x_n \right\}\) and function values
\(\left\{ y_0,\: y_1,\: \dots,\: y_n \right\}\) the polynomial \(P_n\left( x \right)\) defined by:
\begin{equation*}
    P_n\left( x \right) = \sum_{i = 0}^n L_{n,\: i}\left( x \right) y_i
\end{equation*}
where
\begin{equation*}
    L_{n,\: i}\left( x \right) = \prod_{j = 0,\: j \neq i}^n \frac{x - x_j}{x_i - x_j}
\end{equation*}
is called the \textbf{Lagrange form} of the interpolating polynomial.

This arises from the fact that we can construct the Lagrange form using
\begin{align*}
    P_n\left( x \right) & = L_{n,\: 0}\left( x \right) y_0 + L_{n,\: 1}\left( x \right) y_1 + \cdots + L_{n,\: n}\left( x \right) y_n \\
                        & = \sum_{i = 0}^n L_{n,\: i}\left( x \right) y_i
\end{align*}
where the Lagrange basis function is as follows:
\begin{align*}
    L_{n,\: i}\left( x \right) & = \frac{\left( x - x_0 \right)\left( x - x_1 \right) \cdots \left( x - x_{i - 1} \right) \left( x - x_{i + 1} \right) \cdots \left( x - x_n \right)}{\left( x_i - x_0 \right)\left( x_i - x_1 \right) \cdots \left( x_i - x_{i - 1} \right) \left( x_i - x_{i + 1} \right) \cdots \left( x_i - x_n \right)} \\
                               & = \prod_{j = 0,\: j \neq i}^n \frac{x - x_j}{x_i - x_j}
\end{align*}
Note that \(L_{n,\: i}\left( x \right)\) is equal to \textbf{1} when \(x = x_i\), and \textbf{0} for all other abscissa:
\begin{equation*}
    L_{n,\: i}\left( x_j \right) = \delta_{ij}
\end{equation*}
\subsection{Error in Polynomial Approximation}
\begin{theorem}[Lagrange form of remainder]
    Suppose that \(f\) is \(n + 1\) times continuously \linebreak differentiable on \(\interval{a}{b}\)\footnote{\(a = \min \left( x_i \right)\) and \(b = \max \left( x_i \right)\).}
    with distinct abscissas \(x_0,\: x_1,\: \dots,\: x_n\) and interpolating polynomial
    \(P_n\left( x \right)\).

    Then for all \(x \in \interval{a}{b}\) there exists a \(c \in \interval{a}{b}\) such that
    \begin{equation*}
        f\left( x \right) = P_n\left( x \right) + \frac{f^{\left( n + 1 \right)} \left( c \right)}{\left( n + 1 \right)!} \left( x - x_0 \right) \left( x - x_1 \right) \cdots \left( x - x_n \right).
    \end{equation*}
    where the term
    \begin{equation*}
        R_n\left( x \right) = \frac{f^{\left( n + 1 \right)} \left( c \right)}{\left( n + 1 \right)!} \left( x - x_0 \right) \left( x - x_1 \right) \cdots \left( x - x_n \right)
    \end{equation*}
    is called the Lagrange form of the remainder term for \(P_n\left( x \right)\).
\end{theorem}
\subsection{Newton's Divided Difference Form}
While the Lagrange form is straightforward to analyse,
it is not ideal for numerical computation.
Suppose that we have found the interpolating polynomial through
\(n + 1\) points, and are now interested in including a new point
in the interpolation. This requires us to recompute the Lagrange
basis functions for all \(n + 2\) points.
\subsubsection{Derivation}
The Newton form of the interpolating polynomial is given by:
\begin{equation*}
    P_n\left( x \right) = a_0 + a_1 \left( x - x_0 \right) + a_2 \left( x - x_0 \right) \left( x - x_1 \right) + \cdots + a_n \left( x - x_0 \right) \left( x - x_1 \right) \cdots \left( x - x_{n - 1} \right)
\end{equation*}
The interpolation conditions:
\begin{align*}
    P_n\left( x_0 \right) & = y_0 \\
    P_n\left( x_1 \right) & = y_1 \\
    \vdots                        \\
    P_n\left( x_n \right) & = y_n
\end{align*}
give the following system of equations for coefficients \(a_0,\: a_1,\: \dots,\: a_n\):
\begin{align*}
    a_0                                                                                                                                                                                               & = y_0 \\
    a_0 + a_1 \left( x_1 - x_0 \right)                                                                                                                                                                & = y_1 \\
    a_0 + a_1 \left( x_2 - x_0 \right) + a_2 \left( x_2 - x_0 \right) \left( x_2 - x_1 \right)                                                                                                        & = y_2 \\
    \vdots                                                                                                                                                                                            &       \\
    a_0 + a_1 \left( x_n - x_0 \right) + a_2 \left( x_n - x_0 \right) \left( x_n - x_1 \right) + \cdots + a_n \left( x_n - x_0 \right) \left( x_n - x_1 \right) \cdots \left( x_n - x_{n - 1} \right) & = y_n
\end{align*}
Solving for the first three of these coefficients gives:
\begin{align*}
    a_0 & = y_0                                                                         \\
    a_1 & = \frac{y_1 - y_0}{x_1 - x_0}                                                 \\
    a_2 & = \frac{\frac{y_2 - y_1}{x_2 - x_1} - \frac{y_1 - y_0}{x_1 - x_0}}{x_2 - x_0}
\end{align*}
To consisely represent these coefficients, we will need to define divided differences.
\subsubsection{Divided Differences}
\begin{definition}[Zeroth divided difference]
    The zeroth divided difference of \(f\) with respect to \(x_i\) is denoted \(f\left[ x_i \right]\) and defined by:
    \begin{equation*}
        f\left[ x_i \right] = y_i
    \end{equation*}
\end{definition}
\begin{definition}[\(k\)th divided difference]
    The \(k\)th divided difference of \(f\) with respect to \linebreak \(x_i,\: x_{i + 1},\: \dots,\: x_{i + k}\)
    is denoted \(f\left[ x_i,\: x_{i + 1},\: \dots,\: x_{i + k} \right]\) and defined by:
    \begin{equation*}
        f\left[ x_i,\: x_{i + 1},\: \dots,\: x_{i + k} \right] = \frac{f\left[ x_{i + 1},\: \dots,\: x_{i + k} \right] - f\left[ x_i,\: \dots,\: x_{i + k - 1} \right]}{x_{i + k} - x_i}
    \end{equation*}
\end{definition}
For example,
\begin{align*}
    f\left[ x_0 \right]               & = y_0                                                                                                                                                     \\
    f\left[ x_1 \right]               & = y_1                                                                                                                                                     \\
    f\left[ x_0,\: x_1 \right]        & = \frac{f\left[ x_1 \right] - f\left[ x_0 \right]}{x_1 - x_0} = \frac{y_1 - y_0}{x_1 - x_0}                                                               \\
    f\left[ x_1,\: x_2 \right]        & = \frac{f\left[ x_2 \right] - f\left[ x_1 \right]}{x_1 - x_0} = \frac{y_2 - y_1}{x_2 - x_1}                                                               \\
    f\left[ x_0,\: x_1,\: x_2 \right] & = \frac{f\left[ x_1,\: x_2 \right] - f\left[ x_0,\: x_1 \right]}{x_2 - x_0} = \frac{\frac{y_2 - y_1}{x_2 - x_1} - \frac{y_1 - y_0}{x_1 - x_0}}{x_2 - x_0}
\end{align*}
Using this notation we can rewrite the coefficients of \(P_n\) as
\begin{align*}
    a_0 & = f\left[ x_0 \right]                               \\
    a_1 & = f\left[ x_0,\: x_1 \right]                        \\
    a_2 & = f\left[ x_0,\: x_1,\: x_2 \right]                 \\
        & \vdotswithin{=}                                     \\
    a_n & = f\left[ x_0,\: x_1,\: x_2,\: \dots,\: x_n \right]
\end{align*}
so that
\begin{align*}
    P_n\left( x \right) = \begin{aligned}[t]
                               & f\left[ x_0 \right] + f\left[ x_0,\: x_1 \right] \left( x - x_0 \right) + f\left[ x_0,\: x_1,\: x_2 \right] \left( x - x_0 \right) \left( x - x_1 \right) + \cdots \\
                               & + f\left[ x_0,\: x_1,\: x_2,\: \dots,\: x_n \right] \left( x - x_0 \right) \left( x - x_1 \right) \cdots \left( x - x_{n - 1} \right).
                          \end{aligned}
\end{align*}
\begin{definition}[Newton's divided difference form]
    Given the abscissas \(x_0,\: x_1,\: \dots,\: x_n\) and the function values \(y_0,\: y_1,\: \dots,\: y_n\) the polynomial \(P_n\) defined by
    \begin{equation*}
        P_n\left( x \right) = \sum_{k = 0}^n f\left[ x_0,\: x_1,\: \dots,\: x_k \right] \left( \prod_{i = 0}^{k - 1} \left( x - x_i \right) \right)
    \end{equation*}
    is called the \textbf{Newton divided difference form of the interpolating polynomial}.
\end{definition}
\subsection{Newton's Forward Difference Form}
In the case where abscissas are equally spaced, the Newton divided difference form can be simplified.
\begin{definition}[Forward difference operator]
    The \textbf{forward difference operator} \(\Delta\) is defined by:
    \begin{equation*}
        \Delta y_i = y_{i + 1} - y_i
    \end{equation*}
    Higher order forward differences are defined by repeated application:
    \begin{align*}
        \Delta^2 y_i & = \Delta \left( \Delta y_i \right) = \Delta \left( y_{i + 1} - y_i \right) = \Delta y_{i + 1} - \Delta y_i = y_{i + 2} - y_{i + 1} - \left( y_{i + 1} - y_i \right) \\
                     & = y_{i + 2} - 2y_{i + 1} + y_i                                                                                                                                      \\
        \Delta^3 y_i & = \Delta \left( \Delta^2 y_i \right) = \Delta \left( y_{i + 2} - 2y_{i + 1} + y_i \right) = \Delta y_{i + 2} - \Delta y_{i + 1} - \Delta y_i                        \\
                     & = y_{i + 3} - 3y_{i + 2} + 3y_{i + 1} - y_i
    \end{align*}
\end{definition}
\begin{theorem}[Simplified divided differences]
    If \(x_0,\: x_1,\: \dots,\: x_n\) are equally spaced, then
    \begin{equation*}
        f\left[ x_0,\: x_1,\: \dots,\: x_k \right] = \frac{\Delta^k y_0}{k! h^k}
    \end{equation*}
    where \(h = x_{i + 1} - x_i\) is the spacing between the abscissas.
\end{theorem}
Using the above theorem and making the substitution \(x = x_0 + sh\) (so that \(x_i = x_0 + ih\)) where \(s = \frac{x - x_0}{h}\), we can rewrite the Newton divided difference form as
\begin{align*}
    P_n\left( x \right) & = \sum_{k = 0}^n f\left[ x_0,\: x_1,\: \dots,\: x_k \right] \left( \prod_{i = 0}^{k - 1} \left( x - x_i \right) \right)                                                 \\
                        & = \sum_{k = 0}^n \frac{\Delta^k y_0}{k! h^k} \left( \prod_{i = 0}^{k - 1} \left( \left( x_0 + sh \right) - \left( x_0 + ih \right) \right) \right)                      \\
                        & = \sum_{k = 0}^n \frac{\Delta^k y_0}{k! h^k} \left( \prod_{i = 0}^{k - 1} \left( s - i \right)h \right)                                                                 \\
                        & = \sum_{k = 0}^n \frac{\Delta^k y_0}{k! h^k} \left( \prod_{i = 0}^{k - 1} \left( s - i \right) \prod_{i = 0}^{k - 1} h \right)                                          \\
                        & = \sum_{k = 0}^n \frac{\Delta^k y_0}{k! h^k} \left( \prod_{i = 0}^{k - 1} \left( s - i \right) h^k \right)                                                              \\
                        & = \sum_{k = 0}^n \frac{\Delta^k y_0}{k!} s\left( s - 1 \right) \left( s - 2 \right) \cdots \left( s - \left( k - 1 \right) \right)                                      \\
                        & = \sum_{k = 0}^n \frac{\Delta^k y_0}{k!} s\left( s - 1 \right) \left( s - 2 \right) \cdots \left( s - k + 1 \right) \frac{\left( s - k \right)!}{\left( s - k \right)!} \\
                        & = \sum_{k = 0}^n \Delta^k y_0 \frac{s!}{k!\left( s - k \right)!}                                                                                                        \\
                        & = \sum_{k = 0}^n \binom{s}{k} \Delta^k y_0
\end{align*}
\begin{definition}[Newton's forward difference form]
    Given equally spaced abscissas \(x_0,\: x_1,\: \dots,\: x_n\) and the function values \(y_0,\: y_1,\: \dots,\: y_n\) the polynomial \(P_n\) defined by
    \begin{equation*}
        P_n\left( x \right) = \sum_{k = 0}^n \binom{\frac{x - x_0}{h}}{k} \Delta^k y_0
    \end{equation*}
    where \(s = \frac{x - x_0}{h}\) with spacing \(h = x_{i + 1} - x_i\), is called the \textbf{Newton forward difference form of the interpolating polynomial}.
\end{definition}
\section{Root Finding}
Given a function \(f\left( x \right)\), it is often useful to find the values of \(x\) for which \(f\left( x \right) = 0\).
These values are called \textbf{roots} of that function. The process of finding roots is very straightforward
for linear functions of the form \(f\left( x \right) = a x + b\), but becomes more complicated for nonlinear functions.
In these instances, it is often not possible to find the roots analytically, and numerical methods must be used.

These methods are called \textbf{root finding methods}.
\subsection{Bisection Method}
The bisection method is a simple method based on the intermediate value theorem.
\begin{theorem}[Intermediate Value Theorem]
    Let \(f\) be a continuous function on the interval \(\interval{a}{b}\),
    and let \(k\) be any number between \(f\left( a \right)\) and \(f\left( b \right)\) inclusive.
    Then, there exists a number \(c\) in \(\interval{a}{b}\) such that \(f\left( c \right) = k\).
    \begin{equation*}
        f\left( a \right) \leq k \leq f\left( b \right) \iff \exists c \in \interval{a}{b} : f\left( c \right) = k.
    \end{equation*}
\end{theorem}
\begin{corollary}
    Let \(f\) be a continuous function on the interval \(\interval{a}{b}\).
    If \(f\left( a \right) f\left( b \right) < 0\) (\(f\left( a \right)\) and \(f\left( b \right)\) have opposite signs),
    then there exists a number \(c\) in \(\interval{a}{b}\) such that \(f\left( c \right) = 0\).
    \begin{equation*}
        f\left( a \right) f\left( b \right) < 0 \iff \exists c \in \interval{a}{b} : f\left( c \right) = 0.
    \end{equation*}
\end{corollary}
The bisection method is based on the following algorithm:
\begin{enumerate}
    \item Find an interval \(\interval{a}{b}\) such that \(f\left( a \right) f\left( b \right) < 0\)\footnote{This procedure is known as \textbf{bracketing} the root.}.
    \item Find the midpoint \(p\) of the interval \(\interval{a}{b}\) and evaluate \(f\left( p \right)\).
          \begin{itemize}
              \item If \(f\left( p \right) = 0\), then \(p\) is a root of \(f\).
              \item Otherwise, the root lies in either \(\interval{a}{p}\) or \(\interval{p}{b}\).
                    \begin{itemize}
                        \item If \(f\left( a \right) f\left( p \right) < 0\), then \(p\) becomes the new \(b\) and the root lies in \(\interval{a}{p}\).
                        \item If \(f\left( p \right) f\left( b \right) < 0\), then \(p\) becomes the new \(a\) and the root lies in \(\interval{p}{b}\).
                    \end{itemize}
          \end{itemize}
    \item Go to step 2 and repeat until a satisfactory approximation of the root is found.
\end{enumerate}
This can be expressed in a table format:
\begin{table}[H]
    \centering
    \begin{tabular}{ccccccc}
        \toprule
        \textbf{Iteration} & \(a\)      & \(b\)      & \(p = \frac{a + b}{2}\)       & \(f\left( a \right)\)   & \(f\left( b \right)\)   & \(f\left( p \right)\)   \\
        \midrule
        1                  & \(a_0\)    & \(b_0\)    & \(p_0 = \frac{a_0 + b_0}{2}\) & \(f\left( a_0 \right)\) & \(f\left( b_0 \right)\) & \(f\left( p_0 \right)\) \\
        2                  & \(a_1\)    & \(b_1\)    & \(p_1 = \frac{a_1 + b_1}{2}\) & \(f\left( a_1 \right)\) & \(f\left( b_1 \right)\) & \(f\left( p_1 \right)\) \\
        \(\vdots\)         & \(\vdots\) & \(\vdots\) & \(\vdots\)                    & \(\vdots\)              & \(\vdots\)              & \(\vdots\)              \\
        \bottomrule
    \end{tabular}
    % \caption{} % \label{}
\end{table}
\subsection{Fixed-Point Iteration}
Fixed-point iteration is based on rewriting \(f\left( x \right) = 0\) as
\begin{equation*}
    x = g\left( x \right)
\end{equation*}
The equation is solved if we can find a number \(p\) such that \(g\left( p \right) = p\),
called the \textbf{fixed-point} of \(g\).

Using an initial guess, \(x_0\), and computing \(x_2 = g\left( x_1 \right)\), \(x_3 = g\left( x_2 \right)\), and so on,
we can approximate the fixed-point of \(g\). Under certain conditions, the \textbf{sequence}, \(\left\{ x_n \right\}\),
will \textbf{converge} to the fixed-point of \(g\) and thus solve the equation \(f\left( x \right) = 0\).
\begin{theorem}[Brouwer's fixed-point theorem]
    Let \(g\) be a continuous function on \(\interval{a}{b}\) with \(g\left( x \right)\) in \(\interval{a}{b}\)
    for all \(x\). Furthermore, let \(g\) be differentiable on \(\ointerval{a}{b}\) and let a positive constant
    \(k < 1\) exist such that \(\abs*{g'\left( x \right)} \leq k\) for all \(x\) in \(\ointerval{a}{b}\).

    Then, \(g\) has a unique fixed-point in \(\interval{a}{b}\), and the iteration \(x_{n+1} = g\left( x_n \right)\)
    will converge to this point for all initial guesses \(x_0\) in \(\interval{a}{b}\).
    \begin{equation*}
        \left( g\left( x \right) \in \interval{a}{b} : \forall x \in \interval{a}{b} \right) \land \left( \exists k \in \R^+ : k < 1 : \abs*{g'\left( x \right)} \leq k : \forall x \in \ointerval{a}{b} \right) \implies \exists! p \in \interval{a}{b} : g\left( p \right) = p.
    \end{equation*}
\end{theorem}
\subsection{Newton's Method}
Newton's method is one of the most widely used methods for solving nonlinear equations.
It approximates the solution of \(f\left( x \right) = 0\) by finding the root of the tangent line to \(f\) at
each iteration. The next iterate is then the intersection of the tangent line with the \(x\)-axis.

The first degree Taylor polynomial of \(f\) at \(x_n\) gives us the tangent line to \(f\) at \(x_n\):
\begin{equation*}
    f\left( x \right) \approx f\left( x_n \right) + f'\left( x_n \right) \left( x - x_n \right)
\end{equation*}
if we solve this equation, we find:
\begin{align*}
    f\left( x_n \right) + f'\left( x_n \right) \left( x - x_n \right) & = 0                                                      \\
    f'\left( x_n \right) \left( x - x_n \right)                       & = -f\left( x_n \right)                                   \\
    x - x_n                                                           & = -\frac{f\left( x_n \right)}{f'\left( x_n \right)}      \\
    x                                                                 & = x_n - \frac{f\left( x_n \right)}{f'\left( x_n \right)}
\end{align*}
This gives us the sequence:
\begin{equation*}
    x_{n + 1} = x_n - \frac{f\left( x_n \right)}{f'\left( x_n \right)}
\end{equation*}
for \(n \geq 0\).
\subsection{Secant Method}
While Newton's method converges rapidly, it requires the derivative of \(f\) to be known. The secant
method approximates the derivative of \(f\) by using the slope of the secant line between two points,
\(\left( x_{n - 1},\: f\left( x_{n - 1} \right)\right)\) and \(\left( x_n,\: f\left( x_n \right) \right)\).
That is, \(f'\left( x_n \right)\) is approximated by
\begin{equation*}
    f'\left( x_n \right) \approx \frac{f\left( x_n \right) - f\left( x_{n - 1} \right)}{x_n - x_{n - 1}}.
\end{equation*}
This gives us the sequence
\begin{equation*}
    x_{n + 1} = x_n - f\left( x_n \right) \frac{x_n - x_{n - 1}}{f\left( x_n \right) - f\left( x_{n - 1} \right)}
\end{equation*}
for \(n \geq 1\). Note that two initial values are required to start the iteration.
\subsection{Convergence of Fixed-Point Iteration}
When the fixed-point iteration converges, the sequence it generates, \(\left\{ x_n \right\}\), satisfies (for sufficiently large \(n\))
\begin{equation*}
    \abs*{x_{n + 1} - p} \approx \lambda \abs*{x_n - p}
\end{equation*}
where \(p\) is the fixed-point of \(g\) and \(0 < \lambda < 1\). Therefore the \textbf{error} in the solution decreases
by a factor of \(\lambda\) in the long run of each iteration.
\subsection{Convergence of Newton's Method}
When Newton's method converges to the root \(p\), the sequence it generates, \(\left\{ x_n \right\}\), satisfies (for sufficiently large \(n\))
\begin{equation*}
    \abs*{x_{n + 1} - p} \approx \lambda \abs*{x_n - p}^2
\end{equation*}
for \(\lambda > 0\). This means that the number of correct digits approximately \textit{doubles} with each iteration.
\subsection{Convergence of the Secant Method}
When the secant method converges to the root \(p\), the sequence it generates, \(\left\{ x_n \right\}\), satisfies (for sufficiently large \(n\))
\begin{equation*}
    \abs*{x_{n + 1} - p} \approx \lambda \abs*{x_n - p}^r
\end{equation*}
for \(\lambda > 0\) and \(r = \left( 1 + \sqrt{5} \right)/2 \approx 1.618\) (Golden ratio).
Thus the secant method has a slower \textit{rate of convergence} than Newton's method.
\section{Numerical Differentiation}
\subsection{First Derivative}
Often no explicit derivative of a function \(f\) can be found, for example when given a table of
values for \(f\left( x \right)\). In this case, the Taylor polynomial can be used.

Provided the function \(f\) is sufficiently differentiable, we can express the Taylor polynomial of \(f\) at \(x_0\) as
\begin{equation*}
    f\left( x \right) = f\left( x_0 \right) + f'\left( x_0 \right) \left( x - x_0 \right) + \frac{f''\left( x_0 \right)}{2!} \left( x - x_0 \right)^2 + \cdots + \frac{f^{\left( n \right)}\left( x_0 \right)}{n!} \left( x - x_0 \right)^n + \frac{f^{\left( n + 1 \right)}\left( c \right)}{\left( n + 1 \right)!} \left( x - x_0 \right)^{n + 1}.
\end{equation*}
If we let \(h = x - x_0\), we can write this as
\begin{equation*}
    f\left( x_0 + h \right) = f\left( x_0 \right) + h f'\left( x_0 \right) + \frac{h^2}{2!} f''\left( x_0 \right) + \cdots + \frac{h^n}{n!} f^{\left( n \right)}\left( x_0 \right) + \frac{h^{n + 1}}{\left( n + 1 \right)!} f^{\left( n + 1 \right)}\left( c \right).
\end{equation*}
where \(c\) is between \(x_0\) and \(x_0 + h\).
\subsubsection{First Order Approximations}
Using the Taylor polynomial of degree 1, we can obtain a first order approximation for \(f'\left( x_0 \right)\).
The formula
\begin{equation*}
    f'\left( x_0 \right) = \frac{f\left( x_0 + h \right) - f\left( x_0 \right)}{h} - \frac{h}{2} f''\left( c \right)
\end{equation*}
is known as the \textbf{first order forward difference} for \(f'\left( x_0 \right)\). Replacing
\(h\) with \(-h\) gives the \textbf{first order backward difference} for \(f'\left( x_0 \right)\). The
\begin{equation*}
    f'\left( x_0 \right) = \frac{f\left( x_0 \right) - f\left( x_0 - h \right)}{h} + \frac{h}{2} f''\left( c \right).
\end{equation*}
In both cases, the error term is \(\mathcal{O}\left( h \right)\).
\subsubsection{Second Order Approximations}
Consider the Taylor polynomial of degree 2 for \(f\left( x_0 + h \right)\) and \(f\left( x_0 - h \right)\):
\begin{align*}
    f\left( x_0 + h \right) & = f\left( x_0 \right) + h f'\left( x_0 \right) + \frac{h^2}{2!} f''\left( x_0 \right) + \frac{h^3}{3!} f'''\left( c_1 \right) \\
    f\left( x_0 - h \right) & = f\left( x_0 \right) - h f'\left( x_0 \right) + \frac{h^2}{2!} f''\left( x_0 \right) - \frac{h^3}{3!} f'''\left( c_2 \right)
\end{align*}
where \(c_1\) is between \(x_0\) and \(x_0 + h\), and \(c_2\) is between \(x_0 - h\) and \(x_0\).
Subtracting the two equations gives
\begin{align*}
    f\left( x_0 + h \right) - f\left( x_0 - h \right) & = 2h f'\left( x_0 \right) + \frac{h^3}{3!} \left( f'''\left( c_1 \right) + f'''\left( c_2 \right) \right) \\
    f'\left( x_0 \right)                              & = \frac{f\left( x_0 + h \right) - f\left( x_0 - h \right)}{2h} - \frac{h^2}{6} f'''\left( c \right)
\end{align*}
where \(f'''\left( c \right) = \frac{f'''\left( c_1 \right) + f''\left( c_2 \right)}{2}\), for \(c_1 \leq c \leq c_2\) (Intermediate Value Theorem),
and \(c\) is between \(x_0 - h\) and \(x_0 + h\).
This is known as the \textbf{second order central difference} approximation for \(f'\left( x_0 \right)\). The error term is \(\mathcal{O}\left( h^2 \right)\).
\subsection{Second Derivative}
It is also possible to obtain a second order central difference approximation for the second derivative, \(f''\left( x_0 \right)\) using the Taylor polynomial of degree 3
for \(f\left( x_0 + h \right)\) and \(f\left( x_0 - h \right)\):
\begin{align*}
    f\left( x_0 + h \right) & = f\left( x_0 \right) + h f'\left( x_0 \right) + \frac{h^2}{2!} f''\left( x_0 \right) + \frac{h^3}{3!} f'''\left( x \right) + \frac{h^4}{4!} f^{\left( 4 \right)}\left( c_1 \right) \\
    f\left( x_0 - h \right) & = f\left( x_0 \right) - h f'\left( x_0 \right) + \frac{h^2}{2!} f''\left( x_0 \right) - \frac{h^3}{3!} f'''\left( x \right) + \frac{h^4}{4!} f^{\left( 4 \right)}\left( c_2 \right)
\end{align*}
where \(c_1\) and \(c_2\) are between \(x_0\) and \(x_0 + h\) and between \(x_0 - h\) and \(x_0\), respectively.
Adding the two equations gives
\begin{align*}
    f\left( x_0 + h \right) + f\left( x_0 - h \right) & = 2f\left( x_0 \right) + h^2 f''\left( x_0 \right) + \frac{h^4}{4!} \left( f^{\left( 4 \right)}\left( c_1 \right) + f^{\left( 4 \right)}\left( c_2 \right) \right) \\
    f''\left( x_0 \right)                             & = \frac{f\left( x_0 + h \right) - 2f\left( x_0 \right) + f\left( x_0 - h \right)}{h^2} - \frac{h^2}{12} f^{\left( 4 \right)}\left( c \right)
\end{align*}
where \(f^{\left( 4 \right)}\left( c \right) = \frac{f^{\left( 4 \right)}\left( c_1 \right) + f^{\left( 4 \right)}\left( c_2 \right)}{2}\), for \(c_1 \leq c \leq c_2\) (Intermediate Value Theorem),
and \(c\) is between \(x_0 - h\) and \(x_0 + h\).
\subsection{Instability}
When performing numerical differentiation, it is important to realise that as \(h\) decreases, the first and second derivative formulas will require the
subtraction of nearly equal values, which can lead to \textbf{numerical instability}. It is therefore important to choose a value of \(h\)
so that it is not too large, which results in inaccuracies due to truncation error, or too small, which results in inaccuracies due to roundoff error.
\section{Numerical Integration}
Indefinite integration is defined as the anti-derivative of a function \(f\), denoted by \(F\left( x \right)\),
such that
\begin{equation*}
    F'\left( x \right) = f\left( x \right).
\end{equation*}
The definite integral of \(f\) over the interval \(\interval{a}{b}\) is defined as
\begin{equation*}
    I = \int_a^b f\left( x \right) \odif{x}.
\end{equation*}
Numerical integration, also known as \textbf{quadrature}, is concerned with approximating the definite integral of a function \(f\) over a given interval \(\interval{a}{b}\).
It approximates the integral \(\int_a^b f\left( x \right) \odif{x}\) by a weighted sum of function values.
\begin{equation*}
    \int_a^b f\left( x \right) \odif{x} \approx \sum_{i = 0}^n w_i f\left( x_i \right)
\end{equation*}
where \(w_i\) are the weights and \(x_i\) are the abscissas. The primary rule for developing integral approximations is the interpolating polynomial.
\subsection{Trapezoidal Rule}
Consider dividing the interval \(\interval{a}{b}\) into \(n\) subintervals, each of width \(h\). Then \(h = \frac{b - a}{n}\),
and let \(x_i = a + ih\) for \(i = 0, 1, \ldots, n\), so that \(x_0 = a\) and \(x_n = b\).

The \textbf{trapezoidal rule} uses the interpolating polynomial of degree 1 to approximate \(f\left( x \right)\) over each subinterval \(\interval{x_i}{x_{i + 1}}\).
Consider the interpolating polynomial which passes through the points \(\interval{x_0}{y_0}\) and \(\interval{x_1}{y_1}\) where
\(y_0 = f\left( x_0 \right)\) and \(y_1 = f\left( x_1 \right)\). The interpolating polynomial is
\begin{equation*}
    P_1\left( x \right) = y_0 + s \Delta{y_0}
\end{equation*}
using the Newton forward difference form. Using the change of variables \(x = x_0 + s h\), \(\odif{x} = h \odif{s}\), and the limits of integration become \(\interval{0}{1}\).
The integral of \(P_1\) over \(\interval{0}{1}\) is
\begin{align*}
    \int_{x_0}^{x_1} f\left( x \right) \odif{x} & = \int_0^1 P_1\left( x \right) \odif{x}                               \\
                                                & = \int_0^1 \left( y_0 + s \Delta{y_0} \right) h \odif{s}              \\
                                                & = h \int_0^1 y_0 + s \Delta{y_0} \odif{s}                             \\
                                                & = h \left[ y_0 s + \left( y_1 - y_0 \right) \frac{s^2}{2} \right]_0^1 \\
                                                & = h \left[ y_0 + \frac{y_1 - y_0}{2} \right]                          \\
                                                & = \frac{h}{2} \left( y_0 + y_1 \right)
\end{align*}
It follows that:
\begin{equation*}
    \int_{x_{i - 1}}^{x_i} f\left( x \right) \odif{x} \approx \frac{h}{2} \left[ f\left( x_{i - 1} \right) + f\left( x_i \right) \right]
\end{equation*}
for all \(i = 1, 2, \ldots, n\). The trapezoidal rule is therefore
\begin{align*}
    \int_a^b f\left( x \right) \odif{x} & = \int_{x_0}^{x_1} f\left( x \right) \odif{x} + \int_{x_1}^{x_2} f\left( x \right) \odif{x} + \cdots + \int_{x_{n - 1}}^{x_n} f\left( x \right) \odif{x}                                                                                  \\
                                        & \approx \frac{h}{2} \left[ f\left( x_0 \right) + f\left( x_1 \right) \right] + \frac{h}{2} \left[ f\left( x_1 \right) + f\left( x_2 \right) \right] + \cdots + \frac{h}{2} \left[ f\left( x_{n - 1} \right) + f\left( x_n \right) \right] \\
                                        & = \frac{h}{2} \left[ f\left( x_0 \right) + 2 f\left( x_1 \right) + 2 f\left( x_2 \right) + \cdots + 2 f\left( x_{n - 1} \right) + f\left( x_n \right) \right]                                                                             \\
                                        & = \frac{h}{2} \left[ f\left( x_0 \right) + 2 \sum_{i = 1}^{n - 1} f\left( x_i \right) + f\left( x_n \right) \right]
\end{align*}
Therefore, for a twice continuously differentiable function \(f\) on \(\interval{a}{b}\), with \(h = \frac{b - a}{n}\) and \(x_i = a + i h\), for \(i = 0, 1, \ldots, n\),
\begin{equation*}
    \int_a^b f\left( x \right) \odif{x} = \frac{h}{2} \left[ f\left( x_0 \right) + 2 \sum_{i = 1}^{n - 1} f\left( x_i \right) + f\left( x_n \right) \right] - \frac{\left( b - a \right) h^2}{12} f''\left( c \right)
\end{equation*}
where \(c\) is between \(a\) and \(b\). The error in the trapezoidal rule is \(\mathcal{O}\left( h^2 \right)\), and this method is exact for polynomials of degree one or less, as the error term is zero.
\subsection{Simpson's Rule}
The \textbf{Simpson's rule} uses the interpolating polynomial of degree 2 to approximate \(f\left( x \right)\) over each subinterval \(\interval{x_i}{x_{i + 2}}\).
Consider the interpolating polynomial which passes through the points \(\interval{x_0}{y_0}\), \(\interval{x_1}{y_1}\), and \(\interval{x_2}{y_2}\) where
\(y_i = f\left( x_i \right)\). The interpolating polynomial is
\begin{equation*}
    P_2\left( x \right) = y_0 + s \Delta{y_0} + \frac{s\left( s - 1 \right)}{2} \Delta^2{y_0}
\end{equation*}
using the Newton forward difference form. Using the change of variables \(x = x_0 + s h\), \(\odif{x} = h \odif{s}\), and the limits of integration become \(\interval{0}{2}\).
The integral of \(P_2\) over \(\interval{0}{2}\) is
\begin{align*}
    \int_{x_0}^{x_2} f\left( x \right) \odif{x} & = \int_0^2 P_2\left( x \right) \odif{x}                                                                                                                  \\
                                                & = \int_0^2 \left( y_0 + s \Delta{y_0} + \frac{s\left( s - 1 \right)}{2} \Delta^2{y_0} \right) h \odif{s}                                                 \\
                                                & = h \int_0^1 y_0 + s \Delta{y_0} + \left( \frac{1}{2} s^2 - \frac{1}{2} s \right) \Delta^2{y_0} \odif{s}                                                 \\
                                                & = h \left[ y_0 s + \left( y_1 - y_0 \right) \frac{s^2}{2} + \left( \frac{1}{6} s^3 - \frac{1}{4} s^2 \right) \left( y_2 - 2y_1 + y_0 \right) \right]_0^2 \\
                                                & = h \left[ 2 y_0 + 2 \left( y_1 - y_0 \right) + \left( \frac{8}{6} - \frac{1}{4} 4 \right) \left( y_2 - 2y_1 + y_0 \right) \right]                       \\
                                                & = \frac{h}{3} \left( y_0 + 4 y_1 + y_2 \right)
\end{align*}
It follows that:
\begin{equation*}
    \int_{x_{2i - 2}}^{x_{2i}} f\left( x \right) \odif{x} \approx \frac{h}{3} \left[ f\left( x_{2i - 2} \right) + 4f\left( x_{2i - 1} \right) + f\left( x_{2i} \right) \right]
\end{equation*}
for all \(i = 1, 2, \ldots, n/2\). Simpson's rule is therefore
\begin{align*}
    \int_a^b f\left( x \right) \odif{x} & = \int_{x_0}^{x_2} f\left( x \right) \odif{x} + \int_{x_2}^{x_4} f\left( x \right) \odif{x} + \cdots + \int_{x_{n - 2}}^{x_n} f\left( x \right) \odif{x}                                                         \\
                                        & \approx \begin{aligned}[t]
                                                       & \frac{h}{3} \left[ f\left( x_0 \right) + 4f\left( x_1 \right) + f\left( x_2 \right) \right] + \frac{h}{3} \left[ f\left( x_2 \right) + 4f\left( x_3 \right) + f\left( x_4 \right) \right] + \cdots \\
                                                       & + \frac{h}{3} \left[ f\left( x_{n - 2} \right) + 4f\left( x_{n - 1} \right) + f\left( x_n \right) \right]
                                                  \end{aligned} \\
                                        & = \begin{aligned}[t]
                                                 & \frac{h}{3} \left[ f\left( x_0 \right) + 4 \left( f\left( x_1 \right) + f\left( x_3 \right) + \cdots + f\left( x_{n - 1} \right) \right) \right. \\
                                                 & + \left. 2 \left( f\left( x_2 \right) + f\left( x_4 \right) + \cdots + f\left( x_{n - 2} \right) \right) f\left( x_n \right) \right]
                                            \end{aligned}                                              \\
                                        & = \frac{h}{3} \left[ f\left( x_0 \right) + 4 \sum_{i = 1}^{\frac{n}{2}} f\left( x_{2i - 1} \right) + 2 \sum_{i = 1}^{\frac{n}{2} - 1} f\left( x_{2i} \right) + f\left( x_n \right) \right]
\end{align*}
Therefore, for a four times continuously differentiable function \(f\) on \(\interval{a}{b}\), with \(h = \frac{b - a}{n}\) (with \(n\) even) and \(x_i = a + i h\), for \(i = 0, 1, \ldots, n\),
\begin{equation*}
    \int_a^b f\left( x \right) \odif{x} = \frac{h}{3} \left[ f\left( x_0 \right) + 4 \sum_{i = 1}^{\frac{n}{2}} f\left( x_{2i - 1} \right) + 2 \sum_{i = 1}^{\frac{n}{2} - 1} f\left( x_{2i} \right) + f\left( x_n \right) \right] - \frac{\left( b - a \right) h^4}{180} f^{\left( 4 \right)}\left( c \right)
\end{equation*}
where \(c\) is between \(a\) and \(b\). The error in Simpson's rule is \(\mathcal{O}\left( h^4 \right)\), and this method is exact for polynomials of degree three or less, as the error term is zero.
\section{Linear Systems}
Given a linear system with \(n\) knowns and \(n\) unknowns, we can represent this system using matrix notation:
\begin{align*}
    \begin{bmatrix*}
        a_{11} & a_{12} & \cdots & a_{1n} \\
        a_{21} & a_{22} & \cdots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{n1} & a_{n2} & \cdots & a_{nn}
    \end{bmatrix*}
    \begin{bmatrix*}
        x_1 \\
        x_2 \\
        \vdots \\
        x_n
    \end{bmatrix*}
                          & =
    \begin{bmatrix*}
        b_1 \\
        b_2 \\
        \vdots \\
        b_n
    \end{bmatrix*}                       \\
    \symbf{A} \symbfit{x} & = \symbfit{b}
\end{align*}
where \(\symbf{A}\) is the \textbf{coefficient matrix},
\(\symbfit{x}\) is the \textbf{solution vector}, and \(\symbfit{b}\) is the \textbf{right hand side vector}.
Alternatively, this equation can be written using an \textbf{augmented matrix}, by augmenting \(\symbf{A}\) with \(\symbfit{b}\):
\begin{equation*}
    \begin{bmatrix}[cccc|c]
        a_{11} & a_{12} & \cdots & a_{1n} & b_1    \\
        a_{21} & a_{22} & \cdots & a_{2n} & b_2    \\
        \vdots & \vdots & \ddots & \vdots & \vdots \\
        a_{n1} & a_{n2} & \cdots & a_{nn} & b_n
    \end{bmatrix}
\end{equation*}
the vertical bar separates the coefficient matrix from the right hand side vector.
\subsection{Triangular Systems}
A linear system that takes the form
\begin{equation*}
    \begin{bmatrix}[cccc|c]
        a_{11} & a_{12} & \cdots & a_{1n} & b_1    \\
        0      & a_{22} & \cdots & a_{2n} & b_2    \\
        \vdots & \ddots & \ddots & \vdots & \vdots \\
        0      & \cdots & 0      & a_{nn} & b_n
    \end{bmatrix}
\end{equation*}
is known as an \textbf{upper triangular system}. Similarly, a linear system that takes the form
\begin{equation*}
    \begin{bmatrix}[cccc|c]
        a_{11} & 0      & \cdots & 0      & b_1    \\
        a_{21} & a_{22} & \ddots & \vdots & b_2    \\
        \vdots & \vdots & \ddots & 0      & \vdots \\
        a_{n1} & a_{n2} & \cdots & a_{nn} & b_n
    \end{bmatrix}
\end{equation*}
is known as a \textbf{lower triangular system}.
\subsection{Backward Substitution}
When solving an upper triangular system, we can use \textbf{backward substitution} to solve for the unknowns.
This method starts with the last unknown, \(x_n\), and solves for it using the last equation in the system.
Then, we use the second to last equation to solve for \(x_{n - 1}\), and so on.
This leads to the following algorithm:
\begin{align*}
    x_n       & = \frac{b_n}{a_{nn}}                                                                         \\
    x_{n - 1} & = \frac{b_{n - 1} - a_{n - 1, n} x_n}{a_{n - 1, n - 1}}                                      \\
              & \vdotswithin{=}                                                                              \\
    x_i       & = \frac{b_i - a_{in} x_n - a_{i, n - 1} x_{n - 1} - \cdots - a_{i, i + 1} x_{i + 1}}{a_{ii}} \\
              & = \frac{b_i - \sum_{j = i + 1}^n a_{ij} x_j}{a_{ii}}                                         \\
\end{align*}
\subsection{Forward Substitution}
When solving a lower triangular system, we can use \textbf{forward substitution} to solve for the unknowns.
This method starts with the first unknown, \(x_1\), and solves for it using the first equation in the system.
Then, we use the second equation to solve for \(x_2\), and so on.
This leads to the following algorithm:
\begin{align*}
    x_1 & = \frac{b_1}{a_{11}}                                                                 \\
    x_2 & = \frac{b_2 - a_{21} x_1}{a_{22}}                                                    \\
        & \vdotswithin{=}                                                                      \\
    x_i & = \frac{b_i - a_{i, 1} x_1 - a_{i, 2} x_2 - \cdots - a_{i, i - 1} x_{i - 1}}{a_{ii}} \\
        & = \frac{b_i - \sum_{j = 1}^{i - 1} a_{ij} x_j}{a_{ii}}                               \\
\end{align*}
\subsection{LU Decomposition}
The \textbf{LU decomposition} is a method for factoring a square matrix into a lower triangular matrix \(\symbf{L}\) and an upper triangular matrix \(\symbf{U}\):
\begin{equation*}
    \symbf{A} = \symbf{L} \symbf{U}
\end{equation*}
to solve the linear system:
\begin{align*}
    \symbf{A} \symbfit{x}           & = \symbfit{b}                                                         \\
    \symbf{L} \symbf{U} \symbfit{x} & = \symbfit{b}                                                         \\
    \symbf{L} \symbfit{z}           & = \symbfit{b} &  & \left( \symbf{U} \symbfit{x} = \symbfit{z} \right)
\end{align*}
where \(\symbfit{z}\) can be solved using forward substitution, and then \(\symbfit{x}\) can be solved using backward substitution.
The matrices \(\symbf{L}\) and \(\symbf{U}\) can be found by considering the following:
\begin{align*}
    \symbf{A}                            & = \symbf{L} \symbf{U} \\
    \begin{bmatrix*}
        a_{11} & a_{12} & \cdots & a_{1n} \\
        a_{21} & a_{22} & \cdots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{n1} & a_{n2} & \cdots & a_{nn}
    \end{bmatrix*} & =
    \begin{bmatrix*}
        1 & 0      & \cdots & 0      \\
        \ell_{21} & 1 & \ddots & \vdots      \\
        \vdots & \ddots & \ddots & 0 \\
        \ell_{n1} & \cdots & \ell_{n,n-1} & 1
    \end{bmatrix*}
    \begin{bmatrix*}
        u_{11} & u_{12} & \cdots & u_{1n} \\
        0      & u_{22} & \cdots & u_{2n} \\
        \vdots & \ddots & \ddots & \vdots \\
        0      & \cdots & 0      & u_{nn}
    \end{bmatrix*}
\end{align*}
If we perform this product, we get the following:
\begin{align*}
    \begin{bmatrix*}
        a_{11} & a_{12} & \cdots & a_{1n} \\
        a_{21} & a_{22} & \cdots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{n1} & a_{n2} & \cdots & a_{nn}
    \end{bmatrix*} & = \begin{bmatrix*}
                           u_{11}        & u_{12} & \cdots & u_{1n} \\
                           \ell_{21} u_{11} & \ell_{21} u_{12} + u_{22} & \cdots & \ell_{21} u_{1n} + u_{2n} \\
                           \vdots        & \vdots & \ddots & \vdots \\
                           \ell_{n1} u_{11} & \ell_{n1} u_{12} + \ell_{n2} u_{22} & \cdots & \ell_{n1} u_{1n} + \ell_{n2} u_{2n} + \cdots + u_{nn}
                       \end{bmatrix*}
\end{align*}
which allows us to solve for \(u_{ij}\) and \(\ell_{ij}\). This can be done by
considering the first row, then the first column, then the second row, then the second column, and so on.

In general, the equations to consider are, for \(\ell_{ij}\)
\begin{equation*}
    \ell_{ij} = \begin{cases}
        \frac{a_{ij} - \sum_{k = 1}^{j - 1} \ell_{ik} u_{kj}}{u_{jj}} & \text{if \(i > j\)} \\
        1                                                          & \text{if \(i = j\)} \\
        0                                                          & \text{if \(i < j\)} \\
    \end{cases}
\end{equation*}
and for \(u_{ij}\)
\begin{equation*}
    u_{ij} = \begin{cases}
        a_{ij} - \sum_{k = 1}^{i - 1} \ell_{ik} u_{kj} & \text{if \(i \leq j\)} \\
        0                                           & \text{if \(i > j\)}
    \end{cases}
\end{equation*}
\subsection{Cholesky Decomposition}
The \textbf{Cholesky decomposition} is a method for factoring a symmetric, positive definite matrix into a lower triangular matrix \(\symbf{L}\) and its transpose:
\begin{equation*}
    \symbf{A} = \symbf{L} \symbf{L}^T
\end{equation*}
to solve the linear system:
\begin{align*}
    \symbf{A} \symbfit{x}                & = \symbfit{b}                                                              \\
    \symbf{L} \symbf{L}^\top \symbfit{x} & = \symbfit{b}                                                              \\
    \symbf{L} \symbfit{z}                & = \symbfit{b} &  & \left( \symbf{L}^\top \symbfit{x} = \symbfit{z} \right)
\end{align*}
where \(\symbfit{z}\) can be solved using forward substitution, and then \(\symbfit{x}\) can be solved using backward substitution.
The matrix \(\symbf{L}\) can be found by considering the following:
\begin{align*}
    \symbf{A}                            & = \symbf{L} \symbf{L}^\top \\
    \begin{bmatrix*}
        a_{11} & a_{12} & \cdots & a_{1n} \\
        a_{21} & a_{22} & \cdots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{n1} & a_{n2} & \cdots & a_{nn}
    \end{bmatrix*} & =
    \begin{bmatrix*}
        \ell_{11} & 0      & \cdots & 0      \\
        \ell_{21} & \ell_{22} & \ddots & \vdots      \\
        \vdots & \ddots & \ddots & 0 \\
        \ell_{n1} & \cdots & \ell_{n,n-1} & \ell_{nn}
    \end{bmatrix*}
    \begin{bmatrix*}
        \ell_{11} & \ell_{21} & \cdots & \ell_{n1} \\
        0      & \ell_{22} & \cdots & \ell_{n2} \\
        \vdots & \ddots & \ddots & \vdots \\
        0      & \cdots & 0      & \ell_{nn}
    \end{bmatrix*}
\end{align*}
If we perform this product, we get the following:
\begin{align*}
    \begin{bmatrix*}
        a_{11} & a_{12} & \cdots & a_{1n} \\
        a_{21} & a_{22} & \cdots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{n1} & a_{n2} & \cdots & a_{nn}
    \end{bmatrix*} & = \begin{bmatrix*}
                           \ell_{11}^2        & \ell_{11} \ell_{21} & \cdots & \ell_{11} \ell_{n1} \\
                           \ell_{21} \ell_{11} & \ell_{21}^2 + \ell_{22}^2 & \cdots & \ell_{21} \ell_{n1} + \ell_{22} \ell_{n2} \\
                           \vdots        & \vdots & \ddots & \vdots \\
                           \ell_{n1} \ell_{11} & \ell_{n1} \ell_{21} + \ell_{n2} \ell_{22} & \cdots & \ell_{n1}^2 + \ell_{n2}^2 + \cdots + \ell_{nn}^2
                       \end{bmatrix*}
\end{align*}
which allows us to solve for \(\ell_{ij}\). This can be done by solving each column of \(\symbf{L}\).
In general, the equations are
\begin{equation*}
    \ell_{ij} = \begin{cases}
        \frac{1}{\ell_{jj}} \left( a_{ij} - \sum_{k = 1}^{j - 1} \ell_{ik} \ell_{jk} \right) & \text{if \(i > j\)} \\
        \sqrt{a_{jj} - \sum_{k = 1}^{j - 1} \ell_{jk}^2}                               & \text{if \(i = j\)} \\
        0                                                                           & \text{if \(i < j\)} \\
    \end{cases}
\end{equation*}
\end{document}
